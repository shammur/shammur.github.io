---
layout: page
title: MultimodalXplain
description: Interpreting Multimodal Deep Learning Models
img: assets/img/MultimodalX.jpeg
importance: 1
category: QCRI Projects
# related_publications: true
# profiles:
#   - align: left
#     content: Speech 
#   - align: left
#     description: Multimodal  
#   - align: left    
#     description: Codeswitching ASR
#   - align: left
#     description: Telephony ASR    
---
<style>
.bubble {
    display: inline-block;
    padding: 5px 10px;
    margin: 5px;
    border-radius: 15px;
    background-color: #f0f0f0; /* Default background color */
    color: #333;
    font-size: 18px;
    font-weight: bold
}

.bubble.blue {
    background-color: #007bff;
    color: white;
}

.bubble.green {
    background-color: #28a745;
    color: white;
}

.bubble.red {
    background-color: #dc3545;
    color: white;
}

.bubble.gray {
    background-color: #999999;
    color: white;
}

.bubble.pink {
    background-color: #ffdddd;
    color: white;
}
/* .bubble.white { */
    /* background-color: #008080; */
    /* color: white; */
/* } */

</style>

## About

**MultimodalXplain** is a research-driven initiative focused on understanding and interpreting deep learning models that process different input modalities such as - **speech**, **speech+text** and **speech+text+vision** jointly. As AI models grow in complexity, the need for transparency, interpretability, and trust becomes criticalâ€”especially in sensitive domains such as education, healthcare, and accessibility.
